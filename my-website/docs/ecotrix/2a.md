---
sidebar_position: 3
---
# Unbiasedness and Variance

### OLS estimator is Unbiased

We have
$$
\begin{align*}
    \bold{b}&=\bold{(X'X)}^{-1}\bold{X'y},\\
    &=\bold{(X'X)}^{-1}\bold{X'(X}\boldsymbol{\beta + \varepsilon)},\\
    &=\boldsymbol{\beta} + \bold{(X'X)}^{-1}\bold{X'}\boldsymbol{\varepsilon},\\
\end{align*}
$$
taking expectation both sides, we get
$$
\begin{align*}
    \mathbb{E}[\bold{b|X}]&=\mathbb{E}[\boldsymbol{\beta}|\bold{X}] + \mathbb{E}[\bold{(X'X)}^{-1}\bold{X'}\boldsymbol{\varepsilon}|\bold{X}],\\
    &=\boldsymbol{\beta} + \bold{(X'X)}^{-1}\bold{X'}\underbrace{\mathbb{E}[\boldsymbol{\varepsilon}|\bold{X}]}_{=0},\\
    &= \boldsymbol{\beta}.
\end{align*}
$$

### Variance of OLS estimator

We have
$$
\begin{align*}
    \bold{b}&=\boldsymbol{\beta} + \underbrace{\bold{(X'X)}^{-1}\bold{X'}}_{=\bold{A}}\boldsymbol{\varepsilon},\\
    &=\boldsymbol{\beta} + \bold{A}\boldsymbol{\varepsilon},\\
    \implies \bold{b}-\boldsymbol{\beta}&=\bold{A}\boldsymbol{\varepsilon}.
\end{align*}
$$
Since $\bold{b}$ is a vector of dimension $(K \times 1)$
$$
\begin{align*}
    \mathbb{Var}(\bold{b})&=\begin{bmatrix}
\mathbb{Var}(b_0) & \mathbb{Cov}(b_0,b_1) & \mathbb{Cov}(b_0,b_2)&...&\mathbb{Cov}(b_0,b_K)\\
\mathbb{Cov}(b_1,b_0) & \mathbb{Var}(b_1) & \mathbb{Cov}(b_1,b_2)&...&\mathbb{Cov}(b_1,b_K)\\
\mathbb{Cov}(b_2,b_0) & \mathbb{Cov}(b_2,b_1) & \mathbb{Var}(b_2)&...&\mathbb{Cov}(b_2,b_K)\\
.&.&.&...&.\\
.&.&.&...&.\\
.&.&.&...&.\\
\mathbb{Cov}(b_K,b_0) & \mathbb{Cov}(b_K,b_1) & \mathbb{Cov}(b_K,b_2)&...&\mathbb{Var}(b_K)\\
\end{bmatrix}_{(K \times K)},\\
&=
\begin{bmatrix}
\mathbb{E}[(b_0-\beta_0)^2] & \mathbb{E}[(b_0-\beta_0)(b_1-\beta_1)] & \mathbb{E}[(b_0-\beta_0)(b_2-\beta_2)]&...&\mathbb{E}[(b_0-\beta_0)(b_K-\beta_K)]\\
\mathbb{E}[(b_1-\beta_1)(b_0-\beta_0)] & \mathbb{E}[(b_1-\beta_1)^2] & \mathbb{E}[(b_1-\beta_1)(b_2-\beta_2)]&...&\mathbb{E}[(b_1-\beta_1)(b_K-\beta_K)]\\
\mathbb{E}[(b_2-\beta_2)(b_0-\beta_0)] & \mathbb{E}[(b_2-\beta_2)(b_1-\beta_1)] & \mathbb{E}[(b_2-\beta_2)^2]&...&\mathbb{E}[(b_2-\beta_2)(b_K-\beta_K)]\\
.&.&.&...&.\\
.&.&.&...&.\\
.&.&.&...&.\\
\mathbb{E}[(b_K-\beta_K)(b_0-\beta_0)] & \mathbb{E}[(b_K-\beta_K)(b_1-\beta_1)] & \mathbb{E}[(b_K-\beta_K)(b_2-\beta_2)]&...&\mathbb{E}[(b_K-\beta_K)^2]\\
\end{bmatrix}_{(K \times K)},\\
&=\mathbb{E}
\begin{bmatrix}
[(b_0-\beta_0)^2] & [(b_0-\beta_0)(b_1-\beta_1)] & [(b_0-\beta_0)(b_2-\beta_2)]&...&[(b_0-\beta_0)(b_K-\beta_K)]\\
[(b_1-\beta_1)(b_0-\beta_0)] & [(b_1-\beta_1)^2] & [(b_1-\beta_1)(b_2-\beta_2)]&...&[(b_1-\beta_1)(b_K-\beta_K)]\\
[(b_2-\beta_2)(b_0-\beta_0)] & [(b_2-\beta_2)(b_1-\beta_1)] & [(b_2-\beta_2)^2]&...&[(b_2-\beta_2)(b_K-\beta_K)]\\
.&.&.&...&.\\
.&.&.&...&.\\
.&.&.&...&.\\
[(b_K-\beta_K)(b_0-\beta_0)] & [(b_K-\beta_K)(b_1-\beta_1)] & [(b_K-\beta_K)(b_2-\beta_2)]&...&[(b_K-\beta_K)^2]\\
\end{bmatrix}_{(K \times K)},\\
&=\mathbb{E}[(\bold{b}-\boldsymbol{\beta})(\bold{b}-\boldsymbol{\beta})'],\\
&=\mathbb{E}[\bold{A}\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\bold{A}'].
\end{align*}
$$
$$
\begin{align*}
    \mathbb{Var}(\bold{b|X})&=\mathbb{E}[\bold{A}\boldsymbol{\varepsilon}\boldsymbol{\varepsilon'}\bold{A'|X}],\\
    &=\bold{A}\mathbb{E}[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon'}\bold{|X}]\bold{A'}.\\
\end{align*}
$$
Given $\mathbb{E}[\boldsymbol{\varepsilon}]=0$
$$
\begin{align*}
    \mathbb{E}[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon'}]&=\begin{bmatrix}
\mathbb{Var}(\varepsilon_1) & \mathbb{Cov}(\varepsilon_1,\varepsilon_2) & \mathbb{Cov}(\varepsilon_1,\varepsilon_3)&...&\mathbb{Cov}(\varepsilon_1,\varepsilon_n)\\
\mathbb{Cov}(\varepsilon_2,\varepsilon_1) & \mathbb{Var}(\varepsilon_2) & \mathbb{Cov}(\varepsilon_2,\varepsilon_3)&...&\mathbb{Cov}(\varepsilon_2,\varepsilon_n)\\
\mathbb{Cov}(\varepsilon_3,\varepsilon_1) & \mathbb{Cov}(\varepsilon_3,\varepsilon_2) & \mathbb{Var}(\varepsilon_3)&...&\mathbb{Cov}(\varepsilon_3,\varepsilon_n)\\
.&.&.&...&.\\
.&.&.&...&.\\
.&.&.&...&.\\
\mathbb{Cov}(\varepsilon_n,\varepsilon_1) & \mathbb{Cov}(\varepsilon_n,\varepsilon_2) & \mathbb{Cov}(\varepsilon_n,\varepsilon_3)&...&\mathbb{Var}(\varepsilon_n)\\
\end{bmatrix}_{(n \times n)},\\
\mathbb{E}[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon'}\bold{|X}]&=\begin{bmatrix}
\mathbb{Var}(\varepsilon_1|\bold{X}) & \mathbb{Cov}(\varepsilon_1,\varepsilon_2|\bold{X}) & \mathbb{Cov}(\varepsilon_1,\varepsilon_3|\bold{X})&...&\mathbb{Cov}(\varepsilon_1,\varepsilon_n|\bold{X})\\
\mathbb{Cov}(\varepsilon_2,\varepsilon_1|\bold{X}) & \mathbb{Var}(\varepsilon_2|\bold{X}) & \mathbb{Cov}(\varepsilon_2,\varepsilon_3|\bold{X})&...&\mathbb{Cov}(\varepsilon_2,\varepsilon_n|\bold{X})\\
\mathbb{Cov}(\varepsilon_3,\varepsilon_1|\bold{X}) & \mathbb{Cov}(\varepsilon_3,\varepsilon_2|\bold{X}) & \mathbb{Var}(\varepsilon_3|\bold{X})&...&\mathbb{Cov}(\varepsilon_3,\varepsilon_n|\bold{X})\\
.&.&.&...&.\\
.&.&.&...&.\\
.&.&.&...&.\\
\mathbb{Cov}(\varepsilon_n,\varepsilon_1|\bold{X}) & \mathbb{Cov}(\varepsilon_n,\varepsilon_2|\bold{X}) & \mathbb{Cov}(\varepsilon_n,\varepsilon_3|\bold{X})&...&\mathbb{Var}(\varepsilon_n|\bold{X})\\
\end{bmatrix}_{(n \times n)},\\
\mathbb{E}[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon'}\bold{|X}]&=\begin{bmatrix}
    \mathbb{E}[\varepsilon_1^2|\bold{X}] & \mathbb{E}[\varepsilon_1\varepsilon_2|\bold{X}] & \mathbb{E}[\varepsilon_1\varepsilon_3|\bold{X}]&...&\mathbb{E}[\varepsilon_1\varepsilon_n|\bold{X}]\\
    \mathbb{E}[\varepsilon_2\varepsilon_1|\bold{X}] & \mathbb{E}[\varepsilon_2^2|\bold{X}] & \mathbb{E}[\varepsilon_2\varepsilon_3|\bold{X}]&...&\mathbb{E}[\varepsilon_2\varepsilon_n|\bold{X}]\\
    \mathbb{E}[\varepsilon_3\varepsilon_1|\bold{X}] & \mathbb{E}[\varepsilon_3\varepsilon_2|\bold{X}] & \mathbb{E}[\varepsilon_3^2|\bold{X}]&...&\mathbb{E}[\varepsilon_3\varepsilon_n|\bold{X}]\\
    .&.&.&...&.\\
    .&.&.&...&.\\
    .&.&.&...&.\\
    \mathbb{E}[\varepsilon_n\varepsilon_1|\bold{X}] & \mathbb{E}[\varepsilon_n\varepsilon_2|\bold{X}] & \mathbb{E}[\varepsilon_n\varepsilon_3|\bold{X}]&...&\mathbb{E}[\varepsilon_n^2|\bold{X}]\\
\end{bmatrix}_{(n \times n)}.\\
\end{align*}
$$
Under the assumption of **Homoscedasticity** $(\mathbb{E}[\varepsilon_i^2\bold{|X}]=\sigma^2)$ and **non-autocorrelation** $(\mathbb{E}_{i\neq j}[\varepsilon_i\varepsilon_j\bold{|X}]=0)$
$$
\begin{align*}
    \mathbb{E}[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon'}\bold{|X}]&=\sigma^2 \bold{I},\\
    \implies \mathbb{Var}(\bold{b|X})&=\bold{A}\sigma^2 \bold{I}\bold{A'},\\
    &=\bold{(X'X)}^{-1}\bold{X'} \sigma^2 [\bold{(X'X)}^{-1}\bold{X']'},\\
    &=\sigma^2\underbrace{\bold{(X'X)}^{-1}\bold{X'X}}_{=1}[\bold{(X'X)}^{-1}]\bold{'},\\
    &=\sigma^2[\bold{(X'X)}^{-1}]\bold{'},\\
    &=\sigma^2[\bold{(X'X)'}]^{-1},\\
    &=\sigma^2\bold{(X'X)}^{-1}.\\
\end{align*}
$$
We still cannot compute $\mathbb{Var}(\bold{b|X})$ because $\sigma$ is a population parameter and we have to estimate it.

#### Estimating $\sigma^2$

We know that
$$
\mathbb{E}[\varepsilon_i^2]=\sigma^2.
$$
The sample counterpart of $\varepsilon_i$ is $e_i$, defined as $e_i=y_i-x_i'\bold{b}$. An intuitive option for the estimator of $\sigma^2$ could be $\frac{1}{n}\sum_{i=1}^n e_i^2$. It is essential, however, to check whether this estimator is unbiased or not.  
To check, if
$$
\begin{align*}
    \mathbb{E}\Bigg[\frac{1}{n}\sum_{i=1}^n e_i^2\Bigg] & =\sigma^2 .\\
\end{align*}
$$
We know that
$$
\begin{align*}
    \mathbb{E}\Bigg[\frac{1}{n}\sum_{i=1}^n e_i^2\Bigg]&=\mathbb{E}\Bigg[\frac{1}{n}\bold{e'e}\Bigg]=\frac{1}{n}\mathbb{E}[\bold{e'e}], \tag{1}\\
\end{align*}
$$
$\bold{e}$ can be written as $\bold{e=My=M[X}\boldsymbol{\beta + \varepsilon]}=\bold{M}\boldsymbol{\varepsilon}$ as $\bold{MX}=0$, where $\bold{M}=\bold{I_n-X(X'X)^{-1}X'}$. Therefore
$$
\begin{align*}
    \mathbb{E}[\bold{e'e}]&=\mathbb{E}[\bold{(M\boldsymbol{\varepsilon})'(M\boldsymbol{\varepsilon})}],\\
    &=\mathbb{E}[\bold{\boldsymbol{\varepsilon'}M'M\boldsymbol{\varepsilon}}],
\end{align*}
$$
$\bold{M}$ is symmetric $(\bold{M=M'})$ and idempotent $(\bold{M=M^2})$, hence $\bold{M'M=M^2=M}.$ Therefore
$$
\begin{align*}
    \mathbb{E}[\bold{e'e}]&=\mathbb{E}[\bold{\boldsymbol{\varepsilon'}M'M\boldsymbol{\varepsilon}}],\\
    &=\mathbb{E}[\bold{\boldsymbol{\varepsilon'}M\boldsymbol{\varepsilon}}],
\end{align*}
$$
dimension of matrices $\bold{e'e}$ and $\bold{\boldsymbol{\varepsilon'}M\boldsymbol{\varepsilon}}$ is $1\times 1$, therefore
$$
\begin{align*}
    \mathbb{E}[\bold{e'e}]&=\mathbb{E}[\text{Tr}(\bold{\boldsymbol{\varepsilon'}M\boldsymbol{\varepsilon}})],\\
    &=\mathbb{E}[\text{Tr}(\bold{M\boldsymbol{\varepsilon\varepsilon'}})],\\
\end{align*}
$$
we know that $\mathbb{E}[\text{Tr}(\bold{X})]=\text{Tr}(\mathbb{E}[\bold{X}])$ [[How?]](https://macropy.com/Notebooks_Courses/docs/linearalgebra/1#matrices), therefore,
$$
\begin{align*}
    \mathbb{E}[\bold{e'e}]&=\mathbb{E}[\text{Tr}(\bold{M\boldsymbol{\varepsilon\varepsilon'}})]=\text{Tr}(\mathbb{E}[\bold{M\boldsymbol{\varepsilon\varepsilon'}}]),\\
    &=\text{Tr}(\mathbb{E}[\bold{M\boldsymbol{\varepsilon\varepsilon'}}]),\\
    \mathbb{E}[\bold{e'e}|\bold{X}]&=\text{Tr}(\mathbb{E}[\bold{M\boldsymbol{\varepsilon\varepsilon'}}|\bold{X}]),\\
\end{align*}
$$
$\bold{M}$ is a function of $\bold{X}$, therefore
$$
\begin{align*}
    \mathbb{E}[\bold{e'e}|\bold{X}]&=\text{Tr}(\mathbb{E}[\bold{M\boldsymbol{\varepsilon\varepsilon'}}|\bold{X}])=\text{Tr}(\bold{M}\underbrace{\mathbb{E}[\boldsymbol{\varepsilon\varepsilon'}|\bold{X}]}_{=\sigma^2\bold{I}}),\\
    &=\text{Tr}(\bold{M}\sigma^2\bold{I_n}),\\
    &=\sigma^2\text{Tr}(\bold{M}),\\
    &=\sigma^2\text{Tr}(\bold{I_n-X(X'X)^{-1}X'}),\\
    &=\sigma^2\Big\{\text{Tr}(\bold{I_n})-\text{Tr}\bold{(\underbrace{X(X'X)^{-1}}_{\bold{A}}\underbrace{X'}_{\bold{B}})}\Big\},\\
\end{align*}
$$
We know that $\text{Tr}(\bold{AB})=\text{Tr}(\bold{BA})$, therefore
$$
\begin{align*}
    \mathbb{E}[\bold{e'e}|\bold{X}]&=\sigma^2\Big\{\text{Tr}(\bold{I_n})-\text{Tr}\bold{{(X(X'X)^{-1}}X')}\Big\}=\sigma^2\Big\{\text{Tr}(\bold{I_n})-\text{Tr}\bold{{(X'X(X'X)^{-1}})}\Big\},\\
    &=\sigma^2\Big\{\text{Tr}(\bold{I_n})-\text{Tr}\bold{(I_K)}\Big\},\\
    &=\sigma^2(n-K).
\end{align*}
$$
Applying the law of iterated expectations [[Here]](https://macropy.com/Notebooks_Courses/docs/math_stat/Statistics/imp_stats_result#law-of-iterated-expectations)
$$
\mathbb{E}_{\bold{X}}[\mathbb{E}[\bold{e'e}|\bold{X}]]=\mathbb{E}[\bold{e'e}]=\mathbb{E}_{\bold{X}}[\sigma^2(n-K)]=\sigma^2(n-K).
$$
Rewriting $(1)$ again
$$
\begin{align*}
    \mathbb{E}\Bigg[\frac{1}{n}\sum_{i=1}^n e_i^2\Bigg]&=\frac{1}{n}\mathbb{E}[\bold{e'e}]=\frac{1}{n}\sigma^2(n-K),\\
\end{align*}
$$
we can see that $\frac{1}{n}\sum_{i=1}^n e_i^2$ is not the unbiased estimator of $\sigma^2$, but from the above relation we can find the unbiased estimator of $\sigma^2$ which is the following
$$
\begin{align*}
    \frac{1}{n}\mathbb{E}[\bold{e'e}]\frac{n}{(n-K)}&=\sigma^2,\\
    \implies \mathbb{E}\Bigg[\frac{\bold{e'e}}{(n-K)}\Bigg]&=\sigma^2.\\
\end{align*}
$$
Therefore
$$
\hat{\sigma}^2=\frac{\bold{e'e}}{(n-K)}.
$$
Hence
$$
\begin{align*}
    \mathbb{Var}(\bold{b|X})&=\sigma^2\bold{(X'X)}^{-1}\\
    &=\hat{\sigma}^2\bold{(X'X)}^{-1}\\
    &=\frac{\bold{e'e}}{(n-K)}\bold{(X'X)}^{-1}. \hspace{15px}\blacksquare
\end{align*}
$$