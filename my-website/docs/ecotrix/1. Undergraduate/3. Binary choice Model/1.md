---
sidebar_position: 1
---
# Logistic Regression

## Motivation
*Note:* In this notebook, we will treat $y_i$â€‹ as a **binary variable** that can take on only the values 1 or 0.

<div style={{ textAlign: 'justify' }}>
In a linear probability model, the relationship between the predictors and the probability of the binary outcome is assumed to be linear. However, one of the fundamental limitations of the linear probability model is that it can predict probabilities that fall outside the logical bounds of 0 and 1. This happens because, in a linear model, the estimated probability is a linear function of the predictors, and there's nothing inherent in the linear formulation to restrict the predicted probability to lie between 0 and 1.

This limitation can lead to nonsensical predictions in practical applications. For instance, with a sufficiently large positive or negative predictor value, the linear model might predict a probability greater than 1 or less than 0, which is not meaningful in a probabilistic context. Such predictions defy the basic principles of probability and can significantly impair the interpretability and usefulness of the model.

Logistic regression, on the other hand, overcomes this limitation by using a logistic function to model the probability. The logistic function takes any input from the linear combination of predictors and transforms it into a value between 0 and 1. This transformation ensures that the model's outputs are sensible probabilities, regardless of the values of the predictors. Thus, logistic regression is inherently more suitable for modeling probabilities, as it respects the probabilistic boundaries and provides more reliable and interpretable results in scenarios where the dependent variable is binary.
</div>

## Random Utility Basis

Consider the following dataset as an illustration:

| i | Income | Auto Time | Transit Time | Choice   |
|---|--------|-----------|--------------|----------|
| 1 | 35     | 15.4      | 58.2         | Auto     |
| 2 | 45     | 14.2      | 31.0         | Transit  |
| 3 | 37     | 19.6      | 43.6         | Auto     |
| 4 | 42     | 50.8      | 59.9         | Auto     |
| 5 | 32     | 55.5      | 33.8         | Transit  |
| 6 | 15     | 22.3      | 48.4         | Transit  |

*Auto=1, Transit=0.*

* $U_{iA} = \bold{w'}_i \boldsymbol{\beta}_A + \bold{z'}_{iA}\boldsymbol{\gamma}_A + \varepsilon_{iA}$
* $U_{iT} = \bold{w'}_i\boldsymbol{\beta}_T + \bold{z'}_{iT}\boldsymbol{\gamma}_T + \varepsilon_{iT}$.
  * $\underbrace{U_{iA} - U_{iT}}_{y_i^*} =\underbrace{\bold{w'}_i (\boldsymbol{\beta}_A - \boldsymbol{\beta}_T) + (\bold{z'}_{iA}\boldsymbol{\gamma}_A - \bold{z'}_{iT}\boldsymbol{\gamma}_T)}_{\bold{x'}_i\boldsymbol{\beta}} + \underbrace{(\varepsilon_{iA} - \varepsilon_{iT})}_{\varepsilon_i}$
  * $y_i^*=\bold{x'}_i \boldsymbol{\beta} + \varepsilon_i$

    Variables are defined as follows:
    * $U_{iA}:=$ Utility an individual $i$ gets while consuming product $A$ (Auto).
    * $w'_i:=$ Individual $i's$ characteristic like income, sex etc. Note that these characteristics do not vary with products.
    * $z'_{iA}:=$ Attributes of product $A$ and some attributes can vary across individuals, eg. Transit time.




Let $P[y_i=1|\bold{x}_i]$ is the probability that individual $i$ chooses $1$, in our case it is AUTO. Observe the following equivalence:

$$
\begin{align*}
P[y_i=1|\bold{x}_i] &\equiv P[U_{iA}>U_{iT}|\bold{x}_i]\\
&=P[(U_{iA}-U_{iT})>0|\bold{x}_i]\\
&=P[y_i^*>0|\bold{x}_i]\\
&=P[(\bold{x'}_i\boldsymbol{\beta} + \varepsilon_i)>0|\bold{x}_i]\\
&=P[\varepsilon_i>-\bold{x'}_i\boldsymbol{\beta} |\bold{x}_i]
\end{align*}
$$

If the distribution of $\varepsilon_i$ is symmetric, then

$$
P[y_i=1|\bold{x}_i] = P[\varepsilon_i<\bold{x'}_i\boldsymbol{\beta} |\bold{x}_i]=F(\bold{x'}_i\boldsymbol{\beta}),
$$
where $F$ is the cdf of $\varepsilon_i.$

Assume that $\varepsilon_i$ has a logistic distribution with $\mu=0$ and $\sigma^2=\pi^2/3$, then
$$
P[y_i=1|\bold{x}_i] = F(\bold{x'}_i\boldsymbol{\beta})=\frac{e^{\bold{x'}_i\boldsymbol{\beta}}}{1+e^{\bold{x'}_i\boldsymbol{\beta}}}.
$$

>**Aside**
>
> Let $X\sim$ Logistic $(\mu,s)$, where $\mu$ is mean and $s$ is scale. Variance of $X$ is given by $\frac{s^2\pi^2}{3}$ and CDF of $X$ is as follows:
>
>$$
>F(x)=\frac{e^{\frac{x-\mu}{s}}}{1+e^{\frac{x-\mu}{s}}}.
>$$
>In our case, $\mu=0$ and $s=1$, therefore
>$$
>F(\bold{x'}_i\boldsymbol{\beta})=\frac{e^{\bold{x'}_i\boldsymbol{\beta}}}{1+e^{\bold{x'}_i\boldsymbol{\beta}}}.
>$$

## Estimation

Likelihood function, $f_i$
$$
f_i= [F(\bold{x'}_i\boldsymbol{\beta})]^y \cdot [1- F(\bold{x'}_i\boldsymbol{\beta})]^{1-y},
$$
where $y\in\{0,1\}$.

Log likelihood function, $\log f_i$
$$
\log f_i= y_i\log[F(\bold{x'}_i\boldsymbol{\beta})] + (1-y_i)\log[1- F(\bold{x'}_i\boldsymbol{\beta})],
$$
take summation both sides,
$$
\sum_{i=1}^n\log f_i= \sum_{i=1}^n \bigg[y_i\log[F(\bold{x'}_i\boldsymbol{\beta})] + (1-y_i)\log[1- F(\bold{x'}_i\boldsymbol{\beta})]\bigg].
$$
**Find such a $\boldsymbol{\beta}$ that maximizes the above sum.**

## Interpretation of the coefficients

### Partial Effect
We have
$$
\begin{align*}
&P[y_i=1|\bold{x}_i] = F(\bold{x'}_i\boldsymbol{\beta})=\frac{e^{\bold{x'}_i\boldsymbol{\beta}}}{1+e^{\bold{x'}_i\boldsymbol{\beta}}}=\frac{e^{\beta_0+\beta_1 x_1+...+\beta_j x_j+...}}{1+e^{\beta_0+\beta_1 x_1+...+\beta_j x_j+...}}.
\end{align*}
$$
Taking the derivative w.r.t to $x_j$
$$
\begin{align*}
&\frac{\partial(P[y_i=1|\bold{x}_i])}{\partial x_j}=\frac{e^{\beta_0+\beta_1 x_1+...+\beta_j x_j+...}}{(1+e^{\beta_0+\beta_1 x_1+...+\beta_j x_j+...})^2}\cdot \beta_j = F(\bold{x'}_i\boldsymbol{\beta})\cdot[1-F(\bold{x'}_i\boldsymbol{\beta})]\cdot \beta_j
\end{align*}
$$
>**Aside**
>
>Derivative w.r.t to vector $\bold{x}$
>$$
>\frac{\partial F(\bold{x'}\boldsymbol{\beta})}{\partial \bold{x}}=\frac{d F(\bold{x'}\boldsymbol{\beta})}{d (\bold{x'}\boldsymbol{\beta})} \cdot \boldsymbol{\beta}=f(\bold{x'}\boldsymbol{\beta}) \cdot \boldsymbol{\beta}=F(\bold{x'}\boldsymbol{\beta}).[1-F(\bold{x'}\boldsymbol{\beta})]\cdot \boldsymbol{\beta}
>$$

## Important points

1.  We can't run one regression for the following two equations because the attribute columns are different.
  * $U_{iA} = \bold{w'}_i \boldsymbol{\beta}_A + \bold{z'}_{iA}\boldsymbol{\gamma}_A + \varepsilon_{iA}$
  * $U_{iT} = \bold{w'}_i\boldsymbol{\beta}_T + \bold{z'}_{iT}\boldsymbol{\gamma}_T + \varepsilon_{iT}$.

Let's think we can, then we have to transform the above dataset in the following way
   
| individual | Income | Travel Time |  Utility |
|------------|--------|-------------|----------|
| 1          | 35     | 15.4        | $U_{1A}$ |
| 1          | 35     | 58.2        | $U_{1T}$ |
| 2          | 45     | 14.2        | $U_{2A}$ |
| 2          | 45     | 31.0        | $U_{2T}$ |
| 3          | 37     | 19.6        | $U_{3A}$ |
| 3          | 37     | 43.6        | $U_{3T}$ |
| 4          | 42     | 50.8        | $U_{4A}$ |
| 4          | 42     | 59.9        | $U_{4T}$ |
| 5          | 32     | 55.5        | $U_{5A}$ |
| 5          | 32     | 33.8        | $U_{5T}$ |
| 6          | 15     | 22.3        | $U_{6A}$ |
| 6          | 15     | 48.4        | $U_{6T}$ |

Now we can run the following regression
$$
U_{ij} = \bold{w'}_i \boldsymbol{\beta} + \bold{z'}_{ij}\boldsymbol{\gamma} + \varepsilon_{ij}
$$