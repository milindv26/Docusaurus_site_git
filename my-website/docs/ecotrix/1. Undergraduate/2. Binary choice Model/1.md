---
sidebar_position: 1
---
# Logistic Regression

## Motivation
*Note:* In this notebook, we will treat $y_i$â€‹ as a binary variable that can take on only the values 1 or 0.

<div style={{ textAlign: 'justify' }}>
In a linear probability model, the relationship between the predictors and the probability of the binary outcome is assumed to be linear. However, one of the fundamental limitations of the linear probability model is that it can predict probabilities that fall outside the logical bounds of 0 and 1. This happens because, in a linear model, the estimated probability is a linear function of the predictors, and there's nothing inherent in the linear formulation to restrict the predicted probability to lie between 0 and 1.

This limitation can lead to nonsensical predictions in practical applications. For instance, with a sufficiently large positive or negative predictor value, the linear model might predict a probability greater than 1 or less than 0, which is not meaningful in a probabilistic context. Such predictions defy the basic principles of probability and can significantly impair the interpretability and usefulness of the model.

Logistic regression, on the other hand, overcomes this limitation by using a logistic function to model the probability. The logistic function takes any input from the linear combination of predictors and transforms it into a value between 0 and 1. This transformation ensures that the model's outputs are sensible probabilities, regardless of the values of the predictors. Thus, logistic regression is inherently more suitable for modeling probabilities, as it respects the probabilistic boundaries and provides more reliable and interpretable results in scenarios where the dependent variable is binary.
</div>

Consider the following dataset:

| i | Income | Auto Time | Transit Time | Choice   |
|---|--------|-----------|--------------|----------|
| 1 | 35     | 15.4      | 58.2         | Auto     |
| 2 | 45     | 14.2      | 31.0         | Transit  |
| 3 | 37     | 19.6      | 43.6         | Auto     |
| 4 | 42     | 50.8      | 59.9         | Auto     |
| 5 | 32     | 55.5      | 33.8         | Transit  |
| 6 | 15     | 22.3      | 48.4         | Transit  |

* $U_{Ai} = w'_i \beta_A + z'_{Ai}\gamma_A + \varepsilon_{Ai}$

* $U_{Ti} = w'_i\beta_T + z'_{Ti}\gamma_T + \varepsilon_{Ti}$.

$U_{Ai} - U_{Ti} =\underbrace{w'_i (\beta_A - \beta_T) + (z'_{Ai}\gamma_A - z'_{Ti}\gamma_T)}_{X'_i\beta} + \underbrace{(\varepsilon_{Ai} - \varepsilon_{Ti})}_{\varepsilon_i}$

$y_i^*=X'_i\beta + \varepsilon_i$

Observe the following equivalence:

$$
\begin{align*}
P[y_i=1|X_i] \equiv &P[U_{Ai}>U_{Ti}|X_i]\\
&P[(U_{Ai}-U_{Ti})>0|X_i]\\
&P[y_i^*>0|X_i]\\
&P[(X'_i\beta + \varepsilon_i)>0|X_i]\\
&P[\varepsilon_i>-X'_i\beta |X_i]
\end{align*}
$$

If the distribution of $\varepsilon_i$ is symmetric, then

$$
P[y_i=1|X_i] = P[\varepsilon_i<X'_i\beta |X_i].
$$
Assume that $\varepsilon_i$ has a logistic distribution with $\mu=0$ and $\sigma^2=\pi^2/3$, then
$$
P[y_i=1|X_i] = F(X'_i\beta),
$$
where $F$ is the cdf of $\varepsilon_i.$

## Estimation

Likelihood function, $f_i$
$$
f_i= [F(X'_i\beta)]^y \cdot [1- F(X'_i\beta)]^{1-y},
$$
where $y\in\{0,1\}$.

Log likelihood function, $\log f_i$
$$
\log f_i= y_i\log[F(X'_i\beta)] + (1-y_i)\log[1- F(X'_i\beta)],
$$
take summation both sides,
$$
\sum_{i=1}^n\log f_i= \sum_{i=1}^n \bigg[y_i\log[F(X'_i\beta)] + (1-y_i)\log[1- F(X'_i\beta)]\bigg].
$$
Find such a $\beta$ that maximizes the above sum.